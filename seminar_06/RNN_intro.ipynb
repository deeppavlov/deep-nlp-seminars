{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of reproducibility \n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "* [RNN recap](#RNN-recap)\n",
    "    - [Task1](#Task1)\n",
    "    - [Dynamic RNN](#Dynamic-RNN)\n",
    "* [Generate names with RNN](#Name-generation)\n",
    "    - [Task2](#Task2)\n",
    "* [SRU implementation](#SRU-implementation)\n",
    "    - [Task3](#Task3)\n",
    "* [Bonus part](#Bonus-part)\n",
    "* [How to evaluate the work](#How-to-evaluate-the-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN recap\n",
    "\n",
    "<img src=\"./pics/rnn.png\" width=\"90%\">\n",
    "\n",
    "Simplest RNN consisting of 1 layer receives $x_{(t)}$ and could be written as:\n",
    "\n",
    "$$y_{(t)} = \\phi (x_{(t)}^T \\cdot w_x + y_{(t-1)}^T \\cdot w_y + b)$$\n",
    "\n",
    "where \n",
    "* $x(t)$ -- input vector at time step _t_ \n",
    "* $y(t)$ -- output vector at time step _t_\n",
    "* $w_x$ -- weights vector for input \n",
    "* $w_y$ -- weights vector for output\n",
    "* $y(t-1)$ -- output vector at previous time step; for 0th step it's zero vector\n",
    "* $b$ -- bias\n",
    "* $\\phi$ -- some activation function, i.e. ReLU\n",
    "\n",
    "\n",
    "Also we should mention **hidden_state** ( $h(t)$ ) -- it's a recurrent cell memory.\n",
    "\n",
    "In general case $h_{(t)} = f(h_{(t-1)}, x_{(t)})$, but also $y{(t)} = f(h{(t-1)}, x{(t)})$. So in this case $h(t) == y(t)$, but in practice more complex architectures are used, where **hidden_state** is not equal to the RNN output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Lets write simple RNN\n",
    "To write RNN we need to make few improvements to the formula.\n",
    "\n",
    "Lets say that we have not only one vector $x_{(t)}$ as input, but a few vectors in mini-batch $X_{(t)}$ of size $m$ . So all consequent computaions will be in a matrix form.\n",
    "\n",
    "$$ Y_{(t)} = \\phi(X_{(t)} \\cdot W_x + Y_{(t-1)} \\cdot W_y + b) = \\phi([X_{(t)} Y_{(t-1)}] \\cdot W + b) $$\n",
    "where\n",
    "$$ W = [W_x^T W_y^T]^T $$\n",
    "\n",
    "*It's a matrix concatination in square brackets\n",
    "\n",
    "Dimentions:\n",
    "* $Y_{(t)}$ -- matrix [$m$ x n_neurons]\n",
    "* $X_{(t)}$ -- matrix [$m$ x n_features]\n",
    "* $b$ -- vector of size `n_neurons`\n",
    "* $W_x$ -- input weights of size [n_features x n_neurons]\n",
    "* $W_y$ -- output weights of size [n_neurons x n_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph() # just clear default graph and set seed for reproducibility\n",
    "\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "# two time steps\n",
    "# the first dimension in shape parameter is None\n",
    "# because of possibility to feed any sized batch\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_features, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# tanh as phi\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batches of size 4\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])  # time step 0 of mini-batch\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])  # time step 2 mini-batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "Make the same computation using only one matrix multiplication per one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph() # just clear default graph and set seed for reproducibility\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "< your code here >\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val_1, Y1_val_1 = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic RNN\n",
    "\n",
    "In TensorFlow there is a function `tf.contrib.rnn.static_rnn` which create for each time step (unrolling) specific cell of desired type. Our implementation follows `tf.nn.rnn_cell.BasicRNNCell`. This implementation has such a drawback - we could need a lot of memory for long sequences. And because we want to work with such sequences we need to allocate a lot of memory at once. But in TF there is another option -- `dynamic_rnn`, where memory is allocated dynamically for each provided sequence, acording to its length.\n",
    "\n",
    "Lets rewrite the code with `dynamic_rnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always in tensorflow the first step is writing a recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph() # just clear default graph and set seed for reproducibility\n",
    "\n",
    "# adding new parts to the default graph\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "\n",
    "# we have created the same cell in the Task1;\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None]) # create placeholder to feed in real values;\n",
    "\n",
    "# create dynamic_rnn and connect all existing graph components to it (i.e basic_cell, X, seq_length);\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create matrices with real values in numpy.\n",
    "\n",
    "Notice that now `X_batch` have shape = `[None, n_steps, n_features]` that is not the same as in `Task1`.\n",
    "\n",
    "That's because of putting all time steps of batch instances in a single matrix `X_batch` (in the `Task1` we used two separated matrices `X0` and `X1` to feed values in each time stamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "# sequence lengths\n",
    "seq_length_batch = np.array([2, 1, 2, 2]) # note the length of second instance is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed these real values into created network to get outputs and states values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new session context manager;\n",
    "# session will be closed as soon as this cell finish running\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run() # initialize all variables\n",
    "    \n",
    "    # run session and feed input values into the network, get outputs and states values\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `outputs_val` is `[batch_size, time_steps, n_neurons]` as it returns all outputs for each time step for each instance.\n",
    "\n",
    "The shape of `states_val` is `[batch_size, n_neurons]` as it returns only last state for each instance of batch.\n",
    "\n",
    "__For the BasicRNNCell outputs and states are the same.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_val.shape)\n",
    "print(states_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the second sample there are zeros in output \n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but in state there are not\n",
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feed `sequence_length` parameter into the `dynamic_rnn` we make `dynamic_rnn` to stop calculating states after actual sequence is ended. If we don't provide `sequence_length` parameter the calculating of states will continue and useful information about sequence could be lost if the padding is long enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name generation\n",
    "\n",
    "Lets try to do something useful with our RNNs.\n",
    "\n",
    "_Teaser:_\n",
    "\n",
    "* It is hard to choose a name for a variable. But its much harder to choose a name for a person.\n",
    "  So lets make neural net to do it instead!\n",
    "* Dataset consists of 8 thousand people names from different cultures all around the world.\n",
    "* Our toy task is training a model for name generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.readlines()\n",
    "    names = [start_token + name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n samples = ', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x.strip().capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "\n",
    "Lets take all the latters disregarding a case + symbol ')' for the end of a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        token_set.add(letter)\n",
    "\n",
    "\n",
    "token_set.add(')')\n",
    "tokens = list(token_set)\n",
    "tokens.sort()\n",
    "\n",
    "print('n_tokens = ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i: t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len, names)))\n",
    "\n",
    "# max length of a name in this dataset\n",
    "MAX_LEN = min([60, max(list(map(len, names)))])-1\n",
    "\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert symbols to their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get, name + ')')), names))\n",
    "\n",
    "\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN+1] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN+1:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN+1 - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len, names_ix))) == 1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ix[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0, len(data), size=batch_size)]\n",
    "    x = rows[:, :-1]\n",
    "    y = rows[:, 1:]\n",
    "    \n",
    "    count = lambda r: np.sum([id_to_token[t] != ' ' for t in r])\n",
    "    lengths = list(map(count, x))\n",
    "    \n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, length = sample_batch(names_ix, 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture and text generation process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the class `MyLittleNetwork` which will be used to generate sequences.\n",
    "\n",
    "<img src=\"https://vignette.wikia.nocookie.net/mlp/images/4/48/FANMADE_Rainbow_Dash_flying.png/revision/latest?cb=20121227194529\" width=\"100\" align=\"right\">\n",
    "\n",
    "Implemented class will have two useful properties:\n",
    "* Several instances of the class could live in one default graph thanks to using `tf.variable_scope()`\n",
    "* Each class instance could be created with its own recurrent cell type.\n",
    "\n",
    "These properties are useful for us as we want to compare several cell types by creating several class instances.\n",
    "\n",
    "**Outline of our work**\n",
    "\n",
    "1. **[[Build]](#Building-network-graph)** Creating network graph in `MyLittleNetwork.__init__` method\n",
    "2. **[[Train]](#Train-part)** Creating train procedure in `MyLittleNetwork.train` method\n",
    "3. **[[Infer]](#Sequence-generation)** Creating generation procedure in `MyLittleNetwork.generate_sample` method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building network graph\n",
    "\n",
    "Consider method `__init__`. It takes several parameters that will be further discussed.\n",
    "\n",
    "```python\n",
    "...................................................\n",
    "def __init__(self, scope_name,\n",
    "             embedding_size = 8,\n",
    "             cell_class = tf.contrib.rnn.BasicRNNCell,\n",
    "             cell_params_dict = {'num_units': 60, 'activation':tf.tanh},\n",
    "             vocabulary_size = len(tokens)):\n",
    "...................................................         \n",
    "```\n",
    " \n",
    "Here `scope_name` just used to separate graph variables belonging to this particular instance of class `MyLittleNetwork`. `Tf.variable_scope` just add `scope_name` to the full name of all graph variables. In other words `tf.variable_scope` used for namespaces in Tf.\n",
    "We save `scope_name` parameter into `self.scope_name` to use this part of global default_graph along with a particular class instance.\n",
    "\n",
    "In the snippet below we just creating placeholders for inputs `_X`, targets `_y`, sequence length and learning_rate within `scope_name`. We do that using context manager `with tf.variable_scope`.\n",
    "```python\n",
    "...................................................\n",
    "self.scope_name = scope_name\n",
    "with tf.variable_scope(self.scope_name):\n",
    "    self._X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "    self._y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "    self._lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "    self._learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name = 'learning_rate_ph')\n",
    "...................................................\n",
    "```\n",
    "\n",
    "Look at the `_X` placeholder. It says you that it needs int32 input values. And there is no mistake.\n",
    "\n",
    "As input we will feed a sequence of numbers (it's our mapping numbers in dictionary). Actual shape of the `_X` is not defined yet and could be any. But actually it is `(batch_size, max_sequence_length)`.\n",
    "\n",
    "As you may remember from the part about [dynamic_rnn](#Dynamic-RNN) actually we feed into `dynamic_rnn` inputs with the shape `(batch_size, max_sequence_length, n_features)`. But how to get `n_features` dimension of inputs? Actually, we use `tf.embedding_lookup` function to map indices in `_X` to the vectors of embedding matrix. We put these obtained vectors to the `embed` variable.  And `_embedding_mtx` is just a usual `tf.Variable` with shape `(vocab_size, embedding_size)`. \n",
    "\n",
    "After obtaining embedding vectors for input `_X` we feed them into rnn cycle (i.e dynamic_rnn) which returns to us outputs and states (you may remember how it works from the [dynamic_rnn](#Dynamic-RNN) paragraph).   \n",
    "\n",
    "Actually we could use further either `rnn_outputs` or `states` or both to obtain logits. You can try different settings.\n",
    "But the most simple way is just using `rnn_outputs` as it contains information about each time step (and it is more then in `states`). So, use any option to obtain `_pred_logits` (i.e unnormalized scores for each token in the vocabulary).\n",
    "\n",
    "In the last line of the snippet below you have to translate input `_y` to one_hot representation using tf function.\n",
    "```python\n",
    "\n",
    "...................................................\n",
    "self._embedding_mtx = <create matrix of embeddings>\n",
    "embed = < embed the input sequence >\n",
    "\n",
    "self._cell = cell_class(**cell_params_dict)\n",
    "\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(< choose params >)\n",
    "self._pred_logits = < make logits >\n",
    "labels_one_hot = < create one_hot for targets self._y >\n",
    "...................................................\n",
    "```\n",
    "\n",
    "This is the last part of the architecture implementation.\n",
    "\n",
    "`tf.softmax_cross_entropy_with_logits` measures the probability error in discrete classification tasks in which the classes are mutually exclusive. The function calculate `softmax` under unnormilized logits entirely for efficiency. It returns 1-D Tensor of length `batch_size` of the same type as logits with the softmax cross entropy loss. That means it calculates loss for each instance in the batch separately for all vocabulary using formula: $$- \\Sigma y \\cdot log(\\hat{y})$$\n",
    "\n",
    "Since we have one-hot distribution for $y$ the resulted loss (for each instance in batch) takes into account only the logit value of the corresponding right token. Minimizing this loss leads us to maximizing the similarity between distributions of $y$ and it's estimate $\\hat{y}$.\n",
    "\n",
    "But in fact, loss must be scalar value, not tensor. That's why we apply `tf.reduce_mean` function next.\n",
    "\n",
    "Having loss function it is possible to take `AdamOptimizer` and minimize it (i.e calculate gradients giving a particular input and apply them to change network params). That's it and we will do that in the last line of this snippet. Besides we also define `_pred_probas` which is actually used only to generate sequence on the inference stage and doesn't need at train stage.\n",
    "\n",
    "```python\n",
    "....................................................\n",
    "self._stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                        labels=labels_one_hot,\n",
    "                                                        logits=self._pred_logits)\n",
    "self._loss = tf.reduce_mean(self._stepwise_cross_entropy, name='loss')\n",
    "\n",
    "self._pred_probas = tf.nn.softmax(self._pred_logits, name='pred_probas')\n",
    "\n",
    "self._train_op = tf.train.AdamOptimizer(self._learning_rate_ph)\n",
    "                                        .minimize(self._loss, name='train_op')\n",
    "...................................................\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part\n",
    "\n",
    "To make computations run we need to create new tf.Session or use existing one which hasn't been already closed.\n",
    "In this code snippet new session created but it used without context manager (i.e without `with tf.Session() ...`). It is useful to notice that fact as this session will be also used in the inference stage and we don't want to close it just after train finish.\n",
    "\n",
    "As always we initialize variables in this session in this variable scope. \n",
    "```python\n",
    "...................................................\n",
    "\n",
    "def train(self, n_epochs=10, batches_per_epoch = 500, batch_size = 10, lr = 1e-2):\n",
    "\n",
    "    losses = []\n",
    "    self.sess = tf.Session() \n",
    "    with tf.variable_scope(self.scope_name):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "...................................................\n",
    "```\n",
    "\n",
    "In each epoch for each batch we run `_train_op` and get value of `_loss`.\n",
    "\n",
    "All the loss values collected into `losses` list which is returned at the end of training.\n",
    "\n",
    "See the next paragraph to understand `generate_sample` function.\n",
    "\n",
    "```python\n",
    "...................................................\n",
    "for epoch in range(n_epochs):\n",
    "    print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "    print(\"-------\\n\")\n",
    "    avg_cost = 0\n",
    "    for batch in range(batches_per_epoch):\n",
    "        x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "        _, iloss = self.sess.run([self._train_op, self._loss],\n",
    "                                   {self._X: x_,\n",
    "                                    self._y: y_,\n",
    "                                    self._lengths: len_,\n",
    "                                    self._learning_rate_ph: lr})\n",
    "        avg_cost += iloss\n",
    "        losses.append(iloss)\n",
    "\n",
    "    print(\"EPOCH: \", epoch)\n",
    "    print(\"AVERAGE LOSS: \", avg_cost / batches_per_epoch)\n",
    "\n",
    "print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "...................................................\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence generation\n",
    "**Inference stage**\n",
    "\n",
    "<img src=\"http://tommymullaney.com/img/google-hangouts-feature.png\" width=\"400\">\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "* Lets take seed phrase\n",
    "* Feeding it to the network\n",
    "* Predicting next token\n",
    "    * Next token is being sampled from model predicted distribution\n",
    "* Token is added to seed phrase\n",
    "* Repeat (from step 2)\n",
    "\n",
    "\n",
    "**`def generate_sample()`** in the *class `MyLittleNetwork`* actually do that. But it use `numpy` for sampling.\n",
    "So it actually run session to get probability distribution for the last token, then sample with `numpy` from that distribution to get next token. Token then added to the seed phrase and everything starts again from feeding phrase into the network. The picture greatly illustrate the process. Generation ends when the end token {here we use that token `)`} has been sampled or when the max length riched.\n",
    "\n",
    "\n",
    "It could be implemented more effectively using `tf.multinomial` and `tf.while_loop`.\n",
    "You could try to implement this function for generation sequences using tf only. This part of task is challenging and very optional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "\n",
    "Add your code where necessary to create network architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "class MyLittleNetwork:\n",
    "    def __init__(self, scope_name,\n",
    "                 embedding_size = 8,\n",
    "                 cell_class = tf.contrib.rnn.BasicRNNCell,\n",
    "                 cell_params_dict = {'num_units': 60, 'activation':tf.tanh},\n",
    "                 vocabulary_size = len(tokens)):\n",
    "        \n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            \n",
    "            #################### PLACE FOR YOUR CODE  BELOW #########################\n",
    "            \n",
    "            self._X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "            self._y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "            self._lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "            self._learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name = 'learning_rate_ph')\n",
    "\n",
    "            self._embedding_mtx = <create matrix of embeddings>\n",
    "            embed = < embed the input sequence >\n",
    "            \n",
    "            self._cell = cell_class(**cell_params_dict)\n",
    "\n",
    "            rnn_outputs, states = tf.nn.dynamic_rnn(< choose params >)\n",
    "            self._pred_logits = < make logits >\n",
    "            labels_one_hot = < create one_hot for targets self._y >\n",
    "            \n",
    "            ##################### END OF YOUR TASK HERE ##############################\n",
    "\n",
    "            self._stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                                    labels=labels_one_hot,\n",
    "                                                                    logits=self._pred_logits)\n",
    "\n",
    "            self._loss = tf.reduce_mean(self._stepwise_cross_entropy, name='loss')\n",
    "\n",
    "            self._pred_probas = tf.nn.softmax(self._pred_logits, name='pred_probas')\n",
    "\n",
    "            self._train_op = tf.train.AdamOptimizer(self._learning_rate_ph).\n",
    "                                                minimize(self._loss, name='train_op')\n",
    "    \n",
    "    def generate_sample(self, seed_phrase=None, N=MAX_LEN, n_snippets=1):\n",
    "        \"\"\"\n",
    "        If you don't want to reimplement the function with tf\n",
    "                        don't touch it!\n",
    "        \"\"\"\n",
    "        if seed_phrase is None:\n",
    "            seed_phrase = ' '\n",
    "        elif seed_phrase[0].isalpha():\n",
    "            seed_phrase = ' ' + seed_phrase\n",
    "        seed_phrase = seed_phrase.lower()\n",
    "        seed_phrase = np.array([token_to_id[tok] for tok in seed_phrase])\n",
    "        L = len(seed_phrase)\n",
    "        snippets = []\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            for _ in range(n_snippets):\n",
    "                x = np.zeros(N)\n",
    "                x[:len(seed_phrase)] = seed_phrase\n",
    "                for n in range(N - L):\n",
    "                    feed_dict = {self._X: x[:L + n].reshape([1, -1]), self._lengths: [len(x)]}\n",
    "                    p = self.sess.run(self._pred_probas[:, -1], feed_dict=feed_dict).reshape(-1)\n",
    "                    ix = np.random.choice(np.arange(len(tokens)), p=p)\n",
    "                    x[L + n] = ix\n",
    "                snippet = ''.join([id_to_token[idx] for idx in x])\n",
    "                if ')' in snippet:\n",
    "                    upto = snippet.index(')')\n",
    "                    snippet = snippet[:upto]\n",
    "                snippets.append(snippet.strip().capitalize())\n",
    "        return snippets\n",
    "\n",
    "    def train(self, n_epochs=10, batches_per_epoch = 500, batch_size = 10, lr = 1e-2):\n",
    "\n",
    "        losses = []\n",
    "        self.sess = tf.Session() \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "                print(\"-------\\n\")\n",
    "                avg_cost = 0\n",
    "                for batch in range(batches_per_epoch):\n",
    "                    x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "                    _, iloss = self.sess.run([self._train_op, self._loss],\n",
    "                                               {self._X: x_,\n",
    "                                                self._y: y_,\n",
    "                                                self._lengths: len_,\n",
    "                                                self._learning_rate_ph: lr})\n",
    "                    avg_cost += iloss\n",
    "                    losses.append(iloss)\n",
    "\n",
    "                print(\"EPOCH: \", epoch)\n",
    "                print(\"AVERAGE LOSS: \", avg_cost / batches_per_epoch)\n",
    "\n",
    "            print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN = MyLittleNetwork(scope_name=\"BasicRNNCell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which params in the network are trainable?\n",
    "\n",
    "Sometimes it is useful to look at trainable network parameters.\n",
    "\n",
    "* **for comparison**\n",
    " - A specially it is useful to compare one recurrent cell type to another.\n",
    "\n",
    "* **for sanity check**\n",
    " - Another reason is just to check is everything ok in your current default graph. Maybe there're redundant components which are unwanted. They may not be included into train procedure but may litter graph visualisation in tensorboard. Or maybe you forget to set `trainable=False` for your embedding matrix with pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables(scope=myBasicNN.scope_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now train basic rnn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_basic_rnn = myBasicNN.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time loss_hist_basic_rnn = myBasicNN.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time myBasicNN.generate_sample(seed_phrase='Puti', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Q', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Eug', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Lu', n_snippets=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now lets train LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicLSTM = MyLittleNetwork(scope_name=\"BasicLSTMCell\", cell_class=tf.nn.rnn_cell.BasicLSTMCell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check trainable params**\n",
    "\n",
    "If you look at shapes you will see that LSTM has more params then BasicRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables(scope=myBasicLSTM.scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 3 loss_hist_basic_lstm = myBasicLSTM.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time myBasicLSTM.generate_sample(seed_phrase='Puti', n_snippets=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRU implementation\n",
    "\n",
    "There are a lot of different types of recurrent cells.\n",
    "But the $SRU$ [Simple Recurrent Unit] is one created to address the parallelism issue.\n",
    "\n",
    "It was introduced in a paper [**TRAINING RNNS AS FAST AS CNNS**](https://arxiv.org/abs/1709.02755)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Equations from the article**\n",
    "\n",
    "1. $\\tilde{x_{t}} = Wx_t$\n",
    "2. $f_t = \\sigma(W_f x_t + b_f)$\n",
    "3. $r_t = \\sigma(W_r x_t + b_r)$\n",
    "4. $c_t = f_t \\odot c_{t-1} + (1-f_t) \\odot \\tilde{x_{t}}$\n",
    "5. $h_t = r_t \\odot g(c_t) + (1-r_t) \\odot x_t$\n",
    "\n",
    "$\\odot$ -- means point-wise multiplication\n",
    "\n",
    "__Description__\n",
    "\n",
    "\n",
    "Lets look at the first two steps\n",
    "\n",
    "[1] Given an input $x_t$ at time $t$, we compute a linear\n",
    "transformation $\\tilde{x_{t}}$ ...\n",
    "\n",
    "[2] ... and the forget gate $f_t$\n",
    "\n",
    "This computation depends on $x_t$ only, which _enables computing it in parallel_ across all time steps.\n",
    "The forget gate is used to modulate the internal state $c_t$, which is used to compute the output state $h_t$.\n",
    "\n",
    "In the simplest form of the $SRU$ the equation for $h_t$ looks like this:\n",
    "$$h_t = g(c_t)$$\n",
    "where the $g(\\cdot)$ is an activation function. But the authors decided to use **highway connection** (i.e directly use inputs on the higher layers) and they added reset gate [3] to address this. The reset gate is used to compute the output state $h_t$ [5] as a combination of the internal state $g(c_t)$ and the input $x_t$.\n",
    "\n",
    "**Optional question to check yourself:**\n",
    "\n",
    "*does SRUCell have to be as fast on inference stage as it was on the train stage?*\n",
    "\n",
    "## Task3\n",
    "\n",
    "Implemet SRU cell in TF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRUCell(RNNCell):\n",
    "    \"\"\"Simple recurrent unit cell.\n",
    "    The implementation of: https://arxiv.org/abs/1709.02755.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, activation=tf.nn.tanh, reuse=None):\n",
    "        super(SRUCell, self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units \n",
    "        self._activation = activation\n",
    "\n",
    "        self.Wr = tf.Variable(self.init_matrix([self._num_units, self._num_units]))\n",
    "        self.br = tf.Variable(self.init_matrix([self._num_units]))\n",
    "\n",
    "        self.Wf = tf.Variable(self.init_matrix([self._num_units, self._num_units]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self._num_units]))\n",
    "\n",
    "        self.W = tf.Variable(self.init_matrix([self._num_units, self._num_units]))\n",
    "        \n",
    "        # this will be True if we call the cell once\n",
    "        self.used = False\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def call(self, inputs, state, scope=None):\n",
    "        self.__call__(inputs, state, scope)\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"\n",
    "        f - forget gate\n",
    "        r - reset gate\n",
    "        c - final cell\n",
    "        :param inputs:\n",
    "        :param state:\n",
    "        :param scope:\n",
    "        :return: state, cell\n",
    "        \"\"\"\n",
    "\n",
    "        if self.used:\n",
    "            # if self.used then just get projector matrix as it's already exists;\n",
    "            # It is possible to get it in this way because of reuse=True\n",
    "            # As it just gives us existing variable with name='projector'\n",
    "            projected_inputs = tf.layers.dense(inputs, self._num_units, name='projector', reuse=True)\n",
    "        else:\n",
    "            # In case we call the cell for the first time, we create new variable with name='projector';\n",
    "            # Then we use this projector to get inputs with convinient dimensions;\n",
    "            projected_inputs = tf.layers.dense(inputs,  self._num_units, name='projector')\n",
    "            self.used = True\n",
    "            \n",
    "        with variable_scope.variable_scope(scope or type(self).__name__):\n",
    "\n",
    "            # just to clarify\n",
    "            # c_prev = state\n",
    "            <write code to compute f, r, c>\n",
    "\n",
    "            hidden_state = <compute hidden here>\n",
    "\n",
    "            return hidden_state, c\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking\n",
    "\n",
    "Check your implementation by running **Name generation** with this custom cell.\n",
    "\n",
    "* Train `MyLittleNetwork` with your `SRUCell`. If your implementation is right it should work.\n",
    "* Plot loss history of your model on the one plot with `BasicRNNCell` model and `BasicLSTMCell` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySRUModel = MyLittleNetwork(scope_name='BasicSRU', cell_class=SRUCell,\n",
    "                             cell_params_dict={'num_units': 120, 'activation':tf.tanh})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables(scope=mySRUModel.scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "for i in range(3):\n",
    "    sru_loss_history = mySRUModel.train(n_epochs=1)\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop-start\n",
    "print('OVERALL: {}; For one cycle: {}'.format(execution_time, execution_time/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sru_loss_history = mySRUModel.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mySRUModel.generate_sample(seed_phrase='Pur', n_snippets=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=1000):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(running_mean(loss_hist_basic_rnn), label='BasicRNNCell', alpha=0.4)\n",
    "plt.plot(running_mean(loss_hist_basic_lstm), label='BasicLSTMCell', alpha=0.4)\n",
    "plt.plot(running_mean(sru_loss_history), label='SRUCell', alpha=0.4)\n",
    "\n",
    "plt.title(\"Loss history\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus part\n",
    "### Do more interesting stuff\n",
    "\n",
    "* Multi-layer (MultiRNNCell);\n",
    "* Try to generate tweet, using [this](http://study.mokoron.com) dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to evaluate the work\n",
    "\n",
    "**Check if the work meets the requirements below.**\n",
    "\n",
    "Calculate final mark based on collected points.\n",
    "\n",
    "* Code in the task1 contains 2 matrix multiplications at all and produce similar(or the same) result as code without this optimization. **(+2)**\n",
    "* Generated names looks like names; **(+3)**\n",
    "* Model with SRU generates names as well as models above; **(+3)**\n",
    "* There's a plot with loss history at the end; **(+1)**\n",
    "    - loss should decrease smoothly\n",
    "    \n",
    "    \n",
    "* If any **optional tasks** has been done (any of tasks below); **(+1)**\n",
    "\n",
    "    * On the plot there's also model with Multi-layer RNN or smth like this;\n",
    "    * Another dataset has been checked and you see results in the notebook;\n",
    "    * If generate_sample function was rewritten in tf;\n",
    "\n",
    "**Final mark = ( sum of all the points )/2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
