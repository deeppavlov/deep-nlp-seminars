{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation (kind of)\n",
    "\n",
    "The machine translation is old and well-known field in natural language processing. From the 1950s scientists tried to create a model to automatically translate from say French to English. Nowadays it became possible and the attention mechanism takes great part in that. Here the example image with attention map for the neural machine translation of sample phrase:\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab we will concentrate on much simplier task: we will translate from human readable date to machine readable one.\n",
    "\n",
    "To do this we need to get one more concept - Sequence-to-Sequence language modeling.\n",
    "The idea of such architecture is here:\n",
    "\n",
    "<p aling=\"center\">\n",
    "<img src=\"https://talbaumel.github.io/blog/attention/img/encdec.jpg\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "There is an Embeding layer at the bottom, the bidirectional RNN in the middle and softmax as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENCODER_UNITS = 32\n",
    "DECODER_UNITS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use more complex idea that simple seq2seq: we're adding two explicit parts of our network - encoder and decoder (which is applied attention on). The explanatory picture for this idea is below:\n",
    "<p aling=\"center\"><img src=\"https://i.stack.imgur.com/Zwsmz.png\"></p>\n",
    "\n",
    "The lower part of the network is encoding the input to some hidden intermediate representation and the upper part is decoing the hid–≤en represenataion into some readable output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the attention block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    <your code here>\n",
    "    return output_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fianlly lets create a machine traslation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_simple_nmt(in_chars, out_chars):\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    \n",
    "    input_embed = Embedding(in_chars, ENCODER_UNITS * 2, input_length=TIME_STEPS, trainable=True,\n",
    "                            name='embedding')(inputs)\n",
    "    \n",
    "    enc_out = Bidirectional(LSTM(ENCODER_UNITS, return_sequences=True))(input_embed)\n",
    "    dec_out = LSTM(DECODER_UNITS, return_sequences=True)(enc_out)\n",
    "    attention_mul = attention_3d_block(dec_out)\n",
    "    \n",
    "    output = TimeDistributed(Dense(out_chars, activation='softmax'))(attention_mul)\n",
    "   \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate data. It will be dates in different text formats and in fixed output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "fake.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.choice([0,1,2])\n",
    "        if case_change == 1:\n",
    "            human_readable = human_readable.upper()\n",
    "        elif case_change == 2:\n",
    "            human_readable = human_readable.lower()\n",
    "        # if case_change == 0, do nothing\n",
    "\n",
    "        machine_readable = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(n_examples):\n",
    "    \"\"\"\n",
    "        Creates a dataset with n_examples and vocabularies\n",
    "        :n_examples: the number of examples to generate\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_examples)):\n",
    "        h, m, _ = create_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(list(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(list(machine_vocab) + ['<unk>', '<pad>']))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_int(string, lenght, vocab):\n",
    "    if len(string) > lenght:\n",
    "        string = string[:lenght]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < lenght:\n",
    "        rep += [vocab['<pad>']] * (lenght - len(string))\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_string(ints, inv_vocab):\n",
    "    return [inv_vocab[i] for i in ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually generating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 300000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model_simple_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, TIME_STEPS, human_vocab) for i in inputs])\n",
    "targets = [string_to_int(t, TIME_STEPS, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '6 Jun 09', '20th February 2016', 'Thu 11 Jul 2007']\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_examples(m, human_vocab, inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the actual map of attention using the function __get_activations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n",
    "    \"\"\"\n",
    "        Return activation vectors\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "\n",
    "    model_multi_inputs_cond = True\n",
    "    if not isinstance(inp, list):\n",
    "        # only one input! let's wrap it in a list.\n",
    "        inp = [inp]\n",
    "        model_multi_inputs_cond = False\n",
    "\n",
    "    outputs =  # All layer outputs check layer name!\n",
    "\n",
    "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "\n",
    "    # Create list with model inputs\n",
    "\n",
    "    # Learning phase. 0 = Test mode (no dropout or batch normalization)\n",
    "    # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_map(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    \"\"\"\n",
    "        Visualization of attention map\n",
    "    \"\"\"\n",
    "    # encode the string\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "\n",
    "    # get the output sequence\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    predicted_text = np.argmax(prediction[0], axis=-1)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "\n",
    "    text_ = list(text)\n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = predicted_text.index('<pad>')\n",
    "    # get the activation map\n",
    "    attention_vector = get_activations(model, [encoded], layer_name='attention_vec')[0].squeeze()\n",
    "    activation_map = attention_vector[0:output_length, 0:input_length]\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(activation_map, interpolation='nearest', cmap='gray')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention_map(m, human_vocab, inv_machine_vocab, EXAMPLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably see, the default model for this lab is not that good. But you could try to improve it by yourself. You could get better results, like this:\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/6295292/26899949-bbac0c7c-4b9e-11e7-84d6-c2f31166af07.png\" width=\"800\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Acknowledgements__: code based on keras-attention of Philippe Remy\n",
    "\n",
    "URL: https://github.com/philipperemy/keras-visualize-activations\n",
    "\n",
    "The idea of date translation is borrowed from https://github.com/datalogue/keras-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
